{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":12484652,"datasetId":7877793,"databundleVersionId":13062059},{"sourceType":"datasetVersion","sourceId":12479331,"datasetId":7873968,"databundleVersionId":13055728,"isSourceIdPinned":false}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Better version, with first augmentaion, then rest of the work <Final Solution)","metadata":{}},{"cell_type":"code","source":"# Medical Image Classification using Logistic Regression with VGG16 Feature Extraction\n# This code classifies 4 skin diseases: Monkeypox, Pemphigus, Seborrheic keratosis, Squamous cell carcinoma\n\n# !pip install opencv-contrib-python scikit-learn\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom tensorflow.keras.applications import VGG16\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport shutil\nimport cv2\nimport pickle\nimport gc  # For garbage collection\nimport math\nimport time\nfrom sklearn.neighbors import KNeighborsClassifier\n# ========================================================================================\n# SECTION 1: BASIC SETUP AND PATHS\n# ========================================================================================\n\n# Define paths\noriginal_dataset_path = '/kaggle/input/four-diseases-dataset/Dataset'\naugmented_dataset_path = '/kaggle/working/augmented_dataset'\n\n# Set image size and batch size\nimg_size = (224, 224)  # Resize all images to 224x224 (VGG16 input requirement)\nbatch_size = 32        # Load 32 images at a time during training (memory management)\n\n# Class folder names - the 4 diseases we want to classify\ndiseases = ['Monkeypox', 'Pemphigus', 'Seborrheic keratosis', 'Squamous cell carcinoma']\n\n# ========================================================================================\n# SECTION 2: DATA AUGMENTATION TO BALANCE DATASET\n# ========================================================================================\n\nprint(\"üöÄ Starting data augmentation process...\")\n\n# Create output directory\nos.makedirs(augmented_dataset_path, exist_ok=True)\n\n# Define augmentation generator - creates variations of existing images\naugmenter = ImageDataGenerator(\n    rotation_range=20,        # Rotate images by up to 20 degrees\n    width_shift_range=0.1,    # Shift images horizontally by up to 10%\n    height_shift_range=0.1,   # Shift images vertically by up to 10%\n    zoom_range=0.2,          # Zoom in/out by up to 20%\n    horizontal_flip=True,     # Randomly flip images horizontally\n    fill_mode='nearest'       # Fill empty pixels with nearest neighbor values\n)\n\ntarget_total = 500  # Target number of images per class\n\n# Process each disease class\nfor disease in diseases:\n    print(f\"\\nüìÇ Processing {disease}...\")\n    \n    # Set up input and output directories for this disease\n    input_class_dir = os.path.join(original_dataset_path, disease)\n    output_class_dir = os.path.join(augmented_dataset_path, disease)\n    os.makedirs(output_class_dir, exist_ok=True)\n\n    # Get list of all image files in this class\n    image_files = [f for f in os.listdir(input_class_dir) \n                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n    original_count = len(image_files)\n    \n    print(f\"   üìä Found {original_count} original images\")\n    \n    if original_count == 0:\n        print(f\"   ‚ö†Ô∏è  No images found in {disease} folder!\")\n        continue\n\n    # Copy all original images to output directory\n    print(f\"   üì¶ Copying {original_count} original images...\")\n    for img_file in image_files:\n        src = os.path.join(input_class_dir, img_file)\n        dst = os.path.join(output_class_dir, img_file)\n        try:\n            shutil.copy2(src, dst)  # copy2 preserves metadata\n        except Exception as e:\n            print(f\"   ‚ùå Error copying {img_file}: {e}\")\n\n    # Calculate how many augmented images we need\n    extra_needed = max(0, target_total - original_count)\n    \n    if extra_needed == 0:\n        print(f\"   ‚úÖ {disease} already has {original_count} images (>= target)\")\n        continue\n    \n    print(f\"   üîÑ Need to generate {extra_needed} augmented images...\")\n    \n    # Calculate how many augmented versions per original image\n    augs_per_image = max(1, extra_needed // original_count)\n    remaining_augs = extra_needed % original_count\n    \n    count = 0\n    \n    # Generate augmented images\n    for i, img_file in enumerate(image_files):\n        if count >= extra_needed:\n            break\n            \n        img_path = os.path.join(input_class_dir, img_file)\n        \n        try:\n            # Load original image\n            img = load_img(img_path, target_size=img_size)\n            x = img_to_array(img)\n            x = np.expand_dims(x, axis=0)\n\n            # Determine how many augmentations for this image\n            num_augs = augs_per_image + (1 if i < remaining_augs else 0)\n            \n            # Generate augmented versions\n            aug_iter = augmenter.flow(x, batch_size=1)\n            \n            for j in range(num_augs):\n                if count >= extra_needed:\n                    break\n                    \n                try:\n                    aug_img = next(aug_iter)[0].astype('uint8')\n                    aug_pil = Image.fromarray(aug_img)\n                    \n                    # Create unique filename\n                    base_name = os.path.splitext(img_file)[0]\n                    aug_name = f\"{base_name}_aug_{j+1}.jpg\"\n                    aug_path = os.path.join(output_class_dir, aug_name)\n                    \n                    aug_pil.save(aug_path, 'JPEG', quality=95)\n                    count += 1\n                    \n                    if count % 50 == 0:  # Progress update\n                        print(f\"   üìà Generated {count}/{extra_needed} augmented images...\")\n                        \n                except Exception as e:\n                    print(f\"   ‚ùå Error augmenting {img_file} (version {j+1}): {e}\")\n                    continue\n\n        except Exception as e:\n            print(f\"   ‚ùå Error loading {img_file}: {e}\")\n            continue\n    \n    # Final count verification\n    final_count = len([f for f in os.listdir(output_class_dir) \n                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))])\n    print(f\"   ‚úÖ {disease} final count: {final_count} images\")\n    \n    # Clear memory\n    gc.collect()\n\nprint(\"\\nüéâ Data augmentation completed!\")\n\n# ========================================================================================\n# SECTION 3: CREATE DATA GENERATORS WITH AUGMENTED DATASET\n# ========================================================================================\n\nprint(\"\\nüîÑ Creating data generators with augmented dataset...\")\n\n# Create the ImageDataGenerator with normalization and validation split\ndatagen = ImageDataGenerator(\n    rescale=1./255,         # Normalize pixel values from [0,255] to [0,1]\n    validation_split=0.2,   # Reserve 20% of data for validation\n    horizontal_flip=True,   # Additional augmentation for training\n    zoom_range=0.1          # Additional augmentation for training\n)\n\n# Create training generator with augmented dataset\ntrain_generator = datagen.flow_from_directory(\n    augmented_dataset_path,     # Use augmented dataset\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',          # 80% for training\n    shuffle=True\n)\n\n# Create validation generator with augmented dataset\nval_generator = datagen.flow_from_directory(\n    augmented_dataset_path,     # Use augmented dataset\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',        # 20% for validation\n    shuffle=False\n)\n\n# Print class information\nprint(\"\\nüìã Class labels (folder name ‚Üí label index):\")\nprint(\"Training:\", train_generator.class_indices)\nprint(\"Validation:\", val_generator.class_indices)\n\nprint(f\"\\nüìä Dataset Summary:\")\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {val_generator.samples}\")\nprint(f\"Number of classes: {train_generator.num_classes}\")\n\n# ========================================================================================\n# SECTION 4: VERIFY AUGMENTED DATASET\n# ========================================================================================\n\nprint(\"\\nüîç Verifying augmented dataset balance:\")\nfor disease in diseases:\n    disease_dir = os.path.join(augmented_dataset_path, disease)\n    if os.path.exists(disease_dir):\n        count = len([f for f in os.listdir(disease_dir) \n                    if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))])\n        print(f\"   {disease}: {count} images\")\n    else:\n        print(f\"   {disease}: Directory not found!\")\n\nprint(\"\\n‚úÖ Setup complete! Ready for feature extraction and model training.\")\n\n# ========================================================================================\n# SECTION 5: SAMPLE VISUALIZATION (Optional)\n# ========================================================================================\n\ndef visualize_samples(generator, num_samples=8):\n    \"\"\"Visualize sample images from the generator\"\"\"\n    plt.figure(figsize=(15, 8))\n    \n    # Get a batch of images and labels\n    images, labels = next(generator)\n    \n    for i in range(min(num_samples, len(images))):\n        plt.subplot(2, 4, i+1)\n        plt.imshow(images[i])\n        \n        # Get class name from label\n        class_idx = np.argmax(labels[i])\n        class_names = list(generator.class_indices.keys())\n        class_name = class_names[class_idx]\n        \n        plt.title(f'{class_name}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Uncomment to visualize samples\n# print(\"\\nüñºÔ∏è Sample images from training set:\")\n# visualize_samples(train_generator)\n\nprint(\"\\nüéØ Next steps:\")\nprint(\"1. Extract features using VGG16\")\nprint(\"2. Train logistic regression classifier\")\nprint(\"3. Evaluate model performance\")\n\n\n# ========================================================================================\n# SECTION 6: FEATURE EXTRACTION USING VGG16\n# ========================================================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üî¨ FEATURE EXTRACTION WITH VGG16\")\nprint(\"=\"*50)\n\n# Load pre-trained VGG16 model for feature extraction\nprint(\"üì• Loading pre-trained VGG16 model for feature extraction...\")\n\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n# weights='imagenet': Use pre-trained weights from ImageNet dataset\n# include_top=False: Remove the final classification layer (we'll use our own classifier)\n# input_shape: Expected input image size (224x224x3 for RGB)\n\ndef extract_features(directory, sample_size=None):\n    \"\"\"\n    Extract features from images using VGG16\n    \n    Args:\n        directory: Path to folder containing disease subfolders\n        sample_size: Maximum number of images to process per class (None = all)\n    \n    Returns:\n        features: Numpy array of extracted features\n        labels: Numpy array of corresponding class labels\n    \"\"\"\n    features = []  # Store extracted features\n    labels = []    # Store corresponding labels\n    \n    # Process each disease class\n    for class_name in diseases:\n        class_dir = os.path.join(directory, class_name)\n        if not os.path.exists(class_dir):  # Skip if folder doesn't exist\n            print(f\"‚ö†Ô∏è  Warning: {class_dir} does not exist!\")\n            continue\n            \n        # Get list of image files in this class\n        image_files = [f for f in os.listdir(class_dir) \n                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n        \n        # Limit sample size if specified (for faster processing)\n        if sample_size:\n            image_files = image_files[:sample_size]\n        \n        print(f\"üîç Processing {len(image_files)} images from {class_name}...\")\n        \n        # Process each image in this class\n        for img_file in image_files:\n            img_path = os.path.join(class_dir, img_file)\n            try:\n                # Load and preprocess image\n                img = load_img(img_path, target_size=(224, 224))     # Load as 224x224\n                x = img_to_array(img)                                # Convert to array\n                x = np.expand_dims(x, axis=0)                        # Add batch dimension\n                x = tf.keras.applications.vgg16.preprocess_input(x)  # VGG16 specific preprocessing\n                \n                # Extract features using VGG16 (everything except final classification)\n                feature = base_model.predict(x, verbose=0)  # Get feature vector\n                features.append(feature.flatten())         # Flatten to 1D array\n                labels.append(class_name)                   # Store class name\n                \n            except Exception as e:\n                print(f\"‚ùå Error processing {img_file}: {e}\")\n    \n    return np.array(features), np.array(labels)\n\n# Extract features from augmented dataset\nprint(\"\\nüìä Extracting features from augmented dataset...\")\n\n\n\n# Use 200 samples per class for faster processing (you can increase this)\n\nX, y = extract_features(augmented_dataset_path, sample_size=200)\n\nprint(f\"‚úÖ Features extracted: {X.shape}\")  # Shape: (total_samples, feature_dimensions)\nprint(f\"‚úÖ Labels: {len(y)}\")               # Total number of labels\n\n# Check if we have data\nif len(X) == 0:\n    print(\"‚ùå No features extracted! Check your dataset paths and image files.\")\n    exit()\n\n# Encode labels - Convert text labels to numerical labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\nprint(f\"üìã Label mapping:\")\nfor i, class_name in enumerate(label_encoder.classes_):\n    print(f\"   {class_name} ‚Üí {i}\")\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_encoded,           # Features and labels\n    test_size=0.2,          # 20% for testing\n    random_state = 42,        # For reproducible results\n    stratify=y_encoded      # Ensure equal representation of each class in train/test\n)\n\nprint(f\"üìà Training set: {X_train.shape[0]} samples\")\nprint(f\"üìä Testing set: {X_test.shape[0]} samples\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T08:57:40.053505Z","iopub.execute_input":"2025-07-17T08:57:40.054237Z","iopub.status.idle":"2025-07-17T08:58:58.257873Z","shell.execute_reply.started":"2025-07-17T08:57:40.054213Z","shell.execute_reply":"2025-07-17T08:58:58.257160Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting data augmentation process...\n\nüìÇ Processing Monkeypox...\n   üìä Found 100 original images\n   üì¶ Copying 100 original images...\n   üîÑ Need to generate 400 augmented images...\n   üìà Generated 50/400 augmented images...\n   üìà Generated 100/400 augmented images...\n   üìà Generated 150/400 augmented images...\n   üìà Generated 200/400 augmented images...\n   üìà Generated 250/400 augmented images...\n   üìà Generated 300/400 augmented images...\n   üìà Generated 350/400 augmented images...\n   üìà Generated 400/400 augmented images...\n   ‚úÖ Monkeypox final count: 500 images\n\nüìÇ Processing Pemphigus...\n   üìä Found 100 original images\n   üì¶ Copying 100 original images...\n   üîÑ Need to generate 400 augmented images...\n   üìà Generated 50/400 augmented images...\n   üìà Generated 100/400 augmented images...\n   üìà Generated 150/400 augmented images...\n   üìà Generated 200/400 augmented images...\n   üìà Generated 250/400 augmented images...\n   üìà Generated 300/400 augmented images...\n   üìà Generated 350/400 augmented images...\n   üìà Generated 400/400 augmented images...\n   ‚úÖ Pemphigus final count: 500 images\n\nüìÇ Processing Seborrheic keratosis...\n   üìä Found 100 original images\n   üì¶ Copying 100 original images...\n   üîÑ Need to generate 400 augmented images...\n   üìà Generated 50/400 augmented images...\n   üìà Generated 100/400 augmented images...\n   üìà Generated 150/400 augmented images...\n   üìà Generated 200/400 augmented images...\n   üìà Generated 250/400 augmented images...\n   üìà Generated 300/400 augmented images...\n   üìà Generated 350/400 augmented images...\n   üìà Generated 400/400 augmented images...\n   ‚úÖ Seborrheic keratosis final count: 500 images\n\nüìÇ Processing Squamous cell carcinoma...\n   üìä Found 100 original images\n   üì¶ Copying 100 original images...\n   üîÑ Need to generate 400 augmented images...\n   üìà Generated 50/400 augmented images...\n   üìà Generated 100/400 augmented images...\n   üìà Generated 150/400 augmented images...\n   üìà Generated 200/400 augmented images...\n   üìà Generated 250/400 augmented images...\n   üìà Generated 300/400 augmented images...\n   üìà Generated 350/400 augmented images...\n   üìà Generated 400/400 augmented images...\n   ‚úÖ Squamous cell carcinoma final count: 500 images\n\nüéâ Data augmentation completed!\n\nüîÑ Creating data generators with augmented dataset...\nFound 1600 images belonging to 4 classes.\nFound 400 images belonging to 4 classes.\n\nüìã Class labels (folder name ‚Üí label index):\nTraining: {'Monkeypox': 0, 'Pemphigus': 1, 'Seborrheic keratosis': 2, 'Squamous cell carcinoma': 3}\nValidation: {'Monkeypox': 0, 'Pemphigus': 1, 'Seborrheic keratosis': 2, 'Squamous cell carcinoma': 3}\n\nüìä Dataset Summary:\nTraining samples: 1600\nValidation samples: 400\nNumber of classes: 4\n\nüîç Verifying augmented dataset balance:\n   Monkeypox: 500 images\n   Pemphigus: 500 images\n   Seborrheic keratosis: 500 images\n   Squamous cell carcinoma: 500 images\n\n‚úÖ Setup complete! Ready for feature extraction and model training.\n\nüéØ Next steps:\n1. Extract features using VGG16\n2. Train logistic regression classifier\n3. Evaluate model performance\n\n==================================================\nüî¨ FEATURE EXTRACTION WITH VGG16\n==================================================\nüì• Loading pre-trained VGG16 model for feature extraction...\n\nüìä Extracting features from augmented dataset...\nüîç Processing 200 images from Monkeypox...\nüîç Processing 200 images from Pemphigus...\nüîç Processing 200 images from Seborrheic keratosis...\nüîç Processing 200 images from Squamous cell carcinoma...\n‚úÖ Features extracted: (800, 25088)\n‚úÖ Labels: 800\nüìã Label mapping:\n   Monkeypox ‚Üí 0\n   Pemphigus ‚Üí 1\n   Seborrheic keratosis ‚Üí 2\n   Squamous cell carcinoma ‚Üí 3\nüìà Training set: 640 samples\nüìä Testing set: 160 samples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Compact and easy to understand model code","metadata":{}},{"cell_type":"code","source":"# Enhanced ML Models for Medical Image Classification with VGG16 Features\n# Diseases: Monkeypox, Pemphigus, Seborrheic keratosis, Squamous cell carcinoma\n\n# COMPLETELY FIX OpenBLAS threading warnings - MUST BE FIRST\nimport os\nimport sys\n\n# Set ALL possible threading environment variables BEFORE importing numpy/sklearn\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['OPENBLAS_NUM_THREADS'] = '1'\nos.environ['MKL_NUM_THREADS'] = '1'\nos.environ['VECLIB_MAXIMUM_THREADS'] = '1'\nos.environ['NUMEXPR_NUM_THREADS'] = '1'\nos.environ['NUMBA_NUM_THREADS'] = '1'\nos.environ['BLAS_NUM_THREADS'] = '1'\nos.environ['LAPACK_NUM_THREADS'] = '1'\nos.environ['ATLAS_NUM_THREADS'] = '1'\nos.environ['GOTO_NUM_THREADS'] = '1'\nos.environ['ACCELERATE_NUM_THREADS'] = '1'\n\n# Additional OpenBLAS specific settings\nos.environ['OPENBLAS_VERBOSE'] = '0'\nos.environ['OPENBLAS_MAIN_FREE'] = '1'\n\n# Force single-threaded execution\nimport threading\nthreading.current_thread().name = 'MainThread'\n\n# Disable OpenMP if available\ntry:\n    import mkl\n    mkl.set_num_threads(1)\nexcept ImportError:\n    pass\n\n# Import warnings control early\nimport warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, f1_score\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nimport time\nimport joblib\nimport psutil\nfrom contextlib import contextmanager\nimport subprocess\n\n# Additional warning suppression\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\n\n@contextmanager\ndef suppress_blas_warnings():\n    \"\"\"Context manager to completely suppress BLAS warnings\"\"\"\n    # Redirect stderr temporarily\n    import sys\n    from io import StringIO\n    \n    old_stderr = sys.stderr\n    sys.stderr = StringIO()\n    \n    try:\n        yield\n    finally:\n        sys.stderr = old_stderr\n\ndef validate_data(X_train, X_test, y_train, y_test):\n    \"\"\"Validate input data before training\"\"\"\n    print(\"üîç Validating data...\")\n    \n    # Check if data exists\n    if any(data is None for data in [X_train, X_test, y_train, y_test]):\n        raise ValueError(\"‚ùå Missing required data. Please ensure X_train, X_test, y_train, y_test are defined.\")\n    \n    # Check shapes\n    if X_train.shape[1] != X_test.shape[1]:\n        raise ValueError(\"‚ùå Feature dimension mismatch between train and test sets.\")\n    \n    if len(X_train) != len(y_train) or len(X_test) != len(y_test):\n        raise ValueError(\"‚ùå Sample count mismatch between features and labels.\")\n    \n    # Check for NaN values\n    if np.isnan(X_train).any() or np.isnan(X_test).any():\n        print(\"‚ö†Ô∏è Warning: NaN values detected in features. Consider preprocessing.\")\n    \n    print(f\"‚úÖ Data validation passed\")\n    print(f\"   Training samples: {len(X_train)}\")\n    print(f\"   Test samples: {len(X_test)}\")\n    print(f\"   Features: {X_train.shape[1]}\")\n    print(f\"   Classes: {len(np.unique(y_train))}\")\n\ndef get_memory_usage():\n    \"\"\"Get current memory usage\"\"\"\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / 1024 / 1024  # MB\n\ndef logistic_regression_model(X_train, X_test, y_train, y_test, tune_hyperparams=False):\n    \"\"\"\n    Logistic Regression - Linear classifier, fast training\n    Best for: Linearly separable data, baseline model\n    \"\"\"\n    print(\"üîµ Training Logistic Regression...\")\n    start_time = time.time()\n    start_memory = get_memory_usage()\n    \n    try:\n        # Scale features for better performance\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        # Hyperparameter tuning\n        if tune_hyperparams:\n            param_grid = {'C': [0.1, 1.0, 10.0], 'solver': ['liblinear', 'lbfgs']}\n            model = GridSearchCV(LogisticRegression(random_state=42, max_iter=1000), \n                               param_grid, cv=3, n_jobs=1)\n        else:\n            model = LogisticRegression(random_state=42, max_iter=1000, C=1.0)\n        \n        model.fit(X_train_scaled, y_train)\n        \n        # Predictions\n        y_pred = model.predict(X_test_scaled)\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        training_time = time.time() - start_time\n        memory_used = get_memory_usage() - start_memory\n        \n        return model, y_pred, accuracy, f1, training_time, memory_used, scaler\n        \n    except Exception as e:\n        print(f\"‚ùå Error in Logistic Regression: {str(e)}\")\n        return None, None, 0, 0, 0, 0, None\n\ndef random_forest_model(X_train, X_test, y_train, y_test, tune_hyperparams=False):\n    \"\"\"\n    Random Forest - Ensemble of decision trees\n    Best for: Non-linear patterns, feature importance, robust to overfitting\n    \"\"\"\n    print(\"üå≤ Training Random Forest...\")\n    start_time = time.time()\n    start_memory = get_memory_usage()\n    \n    try:\n        # Hyperparameter tuning\n        if tune_hyperparams:\n            param_grid = {'n_estimators': [50, 100], 'max_depth': [10, 20, None]}\n            model = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=1), \n                               param_grid, cv=3, n_jobs=1)\n        else:\n            model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=1)\n        \n        model.fit(X_train, y_train)\n        \n        # Predictions\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        training_time = time.time() - start_time\n        memory_used = get_memory_usage() - start_memory\n        \n        return model, y_pred, accuracy, f1, training_time, memory_used, None\n        \n    except Exception as e:\n        print(f\"‚ùå Error in Random Forest: {str(e)}\")\n        return None, None, 0, 0, 0, 0, None\n\ndef svm_model(X_train, X_test, y_train, y_test, tune_hyperparams=False):\n    \"\"\"\n    Support Vector Machine - Finds optimal decision boundary\n    Best for: High-dimensional data, complex decision boundaries\n    \"\"\"\n    print(\"‚ö° Training SVM...\")\n    start_time = time.time()\n    start_memory = get_memory_usage()\n    \n    try:\n        # Scale features (important for SVM)\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        # Hyperparameter tuning\n        if tune_hyperparams:\n            param_grid = {'C': [0.1, 1.0, 10.0], 'kernel': ['rbf', 'linear']}\n            model = GridSearchCV(SVC(random_state=42, probability=True), \n                               param_grid, cv=3, n_jobs=1)\n        else:\n            model = SVC(kernel='rbf', random_state=42, probability=True)\n        \n        model.fit(X_train_scaled, y_train)\n        \n        # Predictions\n        y_pred = model.predict(X_test_scaled)\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        training_time = time.time() - start_time\n        memory_used = get_memory_usage() - start_memory\n        \n        return model, y_pred, accuracy, f1, training_time, memory_used, scaler\n        \n    except Exception as e:\n        print(f\"‚ùå Error in SVM: {str(e)}\")\n        return None, None, 0, 0, 0, 0, None\n\ndef knn_model(X_train, X_test, y_train, y_test, tune_hyperparams=False):\n    \"\"\"\n    K-Nearest Neighbors - Instance-based learning\n    Best for: Local patterns, simple implementation\n    \"\"\"\n    print(\"üéØ Training K-Nearest Neighbors...\")\n    start_time = time.time()\n    start_memory = get_memory_usage()\n    \n    try:\n        # Scale features\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        # Use context manager to suppress warnings during KNN training\n        with suppress_blas_warnings():\n            # Force single-threaded execution for KNN\n            \n            if tune_hyperparams:\n                param_grid = {'n_neighbors': [3, 5, 7], 'algorithm': ['ball_tree', 'kd_tree']}\n                model = GridSearchCV(\n                    KNeighborsClassifier(n_jobs=1), \n                    param_grid, \n                    cv=3, \n                    n_jobs=1,\n                    verbose=0\n                )\n                \n            else:\n                # Use ball_tree algorithm to avoid OpenBLAS issues\n                model = KNeighborsClassifier(\n                    n_neighbors=5, \n                    n_jobs=1,\n                    algorithm='ball_tree'  # This avoids BLAS operations\n                )\n            \n            # Train with warning suppression\n            model.fit(X_train_scaled, y_train)\n            \n            # Predictions with warning suppression\n            y_pred = model.predict(X_test_scaled)\n        \n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        training_time = time.time() - start_time\n        memory_used = get_memory_usage() - start_memory\n        \n        return model, y_pred, accuracy, f1, training_time, memory_used, scaler\n        \n    except Exception as e:\n        print(f\"‚ùå Error in KNN: {str(e)}\")\n        return None, None, 0, 0, 0, 0, None\n\ndef gradient_boosting_model(X_train, X_test, y_train, y_test, tune_hyperparams=False):\n    \"\"\"\n    Gradient Boosting - Sequential ensemble learning\n    Best for: Complex patterns, high accuracy, feature interactions\n    \"\"\"\n    print(\"üöÄ Training Gradient Boosting...\")\n    start_time = time.time()\n    start_memory = get_memory_usage()\n    \n    try:\n        # Hyperparameter tuning\n        if tune_hyperparams:\n            param_grid = {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.01]}\n            model = GridSearchCV(GradientBoostingClassifier(random_state=42), \n                               param_grid, cv=3, n_jobs=1)\n        else:\n            model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n        \n        model.fit(X_train, y_train)\n        \n        # Predictions\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        training_time = time.time() - start_time\n        memory_used = get_memory_usage() - start_memory\n        \n        return model, y_pred, accuracy, f1, training_time, memory_used, None\n        \n    except Exception as e:\n        print(f\"‚ùå Error in Gradient Boosting: {str(e)}\")\n        return None, None, 0, 0, 0, 0, None\n\ndef naive_bayes_model(X_train, X_test, y_train, y_test, tune_hyperparams=False):\n    \"\"\"\n    Naive Bayes - Probabilistic classifier\n    Best for: Fast training, text classification, baseline\n    \"\"\"\n    print(\"üìä Training Naive Bayes...\")\n    start_time = time.time()\n    start_memory = get_memory_usage()\n    \n    try:\n        # Hyperparameter tuning\n        if tune_hyperparams:\n            param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7]}\n            model = GridSearchCV(GaussianNB(), param_grid, cv=3, n_jobs=1)\n        else:\n            model = GaussianNB()\n        \n        model.fit(X_train, y_train)\n        \n        # Predictions\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        training_time = time.time() - start_time\n        memory_used = get_memory_usage() - start_memory\n        \n        return model, y_pred, accuracy, f1, training_time, memory_used, None\n        \n    except Exception as e:\n        print(f\"‚ùå Error in Naive Bayes: {str(e)}\")\n        return None, None, 0, 0, 0, 0, None\n\ndef decision_tree_model(X_train, X_test, y_train, y_test, tune_hyperparams=False):\n    \"\"\"\n    Decision Tree - Tree-based classifier\n    Best for: Interpretable decisions, feature selection\n    \"\"\"\n    print(\"üå≥ Training Decision Tree...\")\n    start_time = time.time()\n    start_memory = get_memory_usage()\n    \n    try:\n        # Hyperparameter tuning\n        if tune_hyperparams:\n            param_grid = {'max_depth': [10, 20, None], 'min_samples_split': [2, 5]}\n            model = GridSearchCV(DecisionTreeClassifier(random_state=42), \n                               param_grid, cv=3, n_jobs=1)\n        else:\n            model = DecisionTreeClassifier(random_state=42, max_depth=10)\n        \n        model.fit(X_train, y_train)\n        \n        # Predictions\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        training_time = time.time() - start_time\n        memory_used = get_memory_usage() - start_memory\n        \n        return model, y_pred, accuracy, f1, training_time, memory_used, None\n        \n    except Exception as e:\n        print(f\"‚ùå Error in Decision Tree: {str(e)}\")\n        return None, None, 0, 0, 0, 0, None\n    \"\"\"\n    Multi-Layer Perceptron - Neural network classifier\n    Best for: Complex non-linear patterns, large datasets\n    \"\"\"\n    print(\"üß† Training Neural Network...\")\n    start_time = time.time()\n    start_memory = get_memory_usage()\n    \n    try:\n        # Scale features\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        # Hyperparameter tuning\n        if tune_hyperparams:\n            param_grid = {'hidden_layer_sizes': [(50,), (100,), (100, 50)], \n                         'learning_rate_init': [0.01, 0.001]}\n            model = GridSearchCV(MLPClassifier(max_iter=500, random_state=42), \n                               param_grid, cv=3, n_jobs=1)\n        else:\n            model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n        \n        model.fit(X_train_scaled, y_train)\n        \n        # Predictions\n        y_pred = model.predict(X_test_scaled)\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        training_time = time.time() - start_time\n        memory_used = get_memory_usage() - start_memory\n        \n        return model, y_pred, accuracy, f1, training_time, memory_used, scaler\n        \n    except Exception as e:\n        print(f\"‚ùå Error in Neural Network: {str(e)}\")\n        return None, None, 0, 0, 0, 0, None\n\ndef save_model(model, scaler, model_name, save_dir=\"models\"):\n    \"\"\"Save trained model and scaler\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    model_path = os.path.join(save_dir, f\"{model_name.replace(' ', '_')}_model.pkl\")\n    joblib.dump(model, model_path)\n    \n    if scaler:\n        scaler_path = os.path.join(save_dir, f\"{model_name.replace(' ', '_')}_scaler.pkl\")\n        joblib.dump(scaler, scaler_path)\n    \n    print(f\"üíæ Model saved: {model_path}\")\n\ndef load_model(model_name, save_dir=\"models\"):\n    \"\"\"Load trained model and scaler\"\"\"\n    model_path = os.path.join(save_dir, f\"{model_name.replace(' ', '_')}_model.pkl\")\n    scaler_path = os.path.join(save_dir, f\"{model_name.replace(' ', '_')}_scaler.pkl\")\n    \n    model = joblib.load(model_path)\n    scaler = joblib.load(scaler_path) if os.path.exists(scaler_path) else None\n    \n    print(f\"üìÇ Model loaded: {model_path}\")\n    return model, scaler\n\ndef plot_feature_importance(model, model_name, feature_names=None):\n    \"\"\"Plot feature importance for tree-based models\"\"\"\n    if hasattr(model, 'feature_importances_'):\n        plt.figure(figsize=(10, 6))\n        \n        if hasattr(model, 'best_estimator_'):\n            importances = model.best_estimator_.feature_importances_\n        else:\n            importances = model.feature_importances_\n        \n        # Get top 20 features\n        indices = np.argsort(importances)[::-1][:20]\n        \n        plt.bar(range(len(indices)), importances[indices])\n        plt.title(f'Top 20 Feature Importance - {model_name}')\n        plt.xlabel('Feature Index')\n        plt.ylabel('Importance')\n        plt.xticks(range(len(indices)), indices)\n        plt.tight_layout()\n        plt.show()\n\ndef enhanced_performance_summary(results):\n    \"\"\"Enhanced performance summary with multiple metrics\"\"\"\n    print(\"\\nüìä ENHANCED PERFORMANCE SUMMARY\")\n    print(\"=\"*60)\n    \n    # Create DataFrame for better formatting\n    import pandas as pd\n    \n    df_data = []\n    for model_name, result in results.items():\n        if result['accuracy'] > 0:  # Only include successful models\n            df_data.append({\n                'Model': model_name,\n                'Accuracy': f\"{result['accuracy']:.4f}\",\n                'F1-Score': f\"{result['f1']:.4f}\",\n                'Time (s)': f\"{result['time']:.2f}\",\n                'Memory (MB)': f\"{result['memory']:.1f}\"\n            })\n    \n    df = pd.DataFrame(df_data)\n    df = df.sort_values('Accuracy', ascending=False)\n    print(df.to_string(index=False))\n    \n    return df\n\ndef completely_disable_blas_warnings():\n    \"\"\"Nuclear option: Completely disable all BLAS-related warnings\"\"\"\n    # Redirect stderr to devnull during problematic operations\n    import sys\n    from contextlib import redirect_stderr\n    from io import StringIO\n    \n    # Set additional environment variables\n    os.environ['PYTHONHASHSEED'] = '0'\n    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n    \n    # Suppress all warnings\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    warnings.simplefilter(\"ignore\")\n    \n    # Monkey patch numpy's warning system\n    try:\n        import numpy as np\n        np.seterr(all='ignore')\n    except:\n        pass\n    \n    print(\"üîá All BLAS warnings completely disabled\")\n\ndef main_evaluation(tune_hyperparams=False, save_models=False):\n    \"\"\"Main function to train and evaluate all models\"\"\"\n    print(\"üéØ ENHANCED ML MODEL EVALUATION\")\n    print(\"=\"*50)\n    \n    # Completely disable warnings first\n    completely_disable_blas_warnings()\n    \n    # Validate input data\n    try:\n        validate_data(X_train, X_test, y_train, y_test)\n    except NameError:\n        print(\"‚ùå Please ensure X_train, X_test, y_train, y_test and label_encoder are defined\")\n        return None, None\n    \n    # Get class names\n    class_names = label_encoder.classes_\n    \n    # Dictionary to store all models\n    models_data = {}\n    results = {}\n    \n    # Define ALL models to train\n    model_functions = {\n        'Logistic Regression': logistic_regression_model,\n        'Random Forest': random_forest_model,\n        'SVM': svm_model,\n        'K-Nearest Neighbors': knn_model,\n        # 'Gradient Boosting': gradient_boosting_model,\n        'Naive Bayes': naive_bayes_model,\n        'Decision Tree': decision_tree_model,\n        'Neural Network': neural_network_model\n    }\n    \n    # Train all models\n    for model_name, model_func in model_functions.items():\n        print(f\"\\n{'='*20} {model_name} {'='*20}\")\n        \n        # Train model\n        model, y_pred, accuracy, f1, training_time, memory_used, scaler = model_func(\n            X_train, X_test, y_train, y_test, tune_hyperparams\n        )\n        \n        if model is not None:\n            # Store results\n            models_data[model_name] = (model, scaler)\n            results[model_name] = {\n                'accuracy': accuracy,\n                'f1': f1,\n                'time': training_time,\n                'memory': memory_used,\n                'predictions': y_pred\n            }\n            \n            # Print results\n            print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n            print(f\"‚úÖ F1-Score: {f1:.4f}\")\n            print(f\"‚è±Ô∏è Training Time: {training_time:.2f} seconds\")\n            print(f\"üíæ Memory Used: {memory_used:.1f} MB\")\n            \n            # Save model if requested\n            if save_models:\n                save_model(model, scaler, model_name)\n            \n            # Plot feature importance for applicable models\n            if model_name in ['Random Forest', 'Gradient Boosting', 'Decision Tree']:\n                plot_feature_importance(model, model_name)\n    \n    # Enhanced performance summary\n    enhanced_performance_summary(results)\n    \n    return models_data, results\n\n# Example usage with complete warning suppression\nif __name__ == \"__main__\":\n    # Disable warnings before running\n    completely_disable_blas_warnings()\n    \n    # Run with hyperparameter tuning and model saving\n    print(\"üöÄ Starting model evaluation with all warnings suppressed...\")\n    models_data, results = main_evaluation(tune_hyperparams=True, save_models=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:07:20.664524Z","iopub.execute_input":"2025-07-17T10:07:20.665243Z"}},"outputs":[{"name":"stdout","text":"üîá All BLAS warnings completely disabled\nüöÄ Starting model evaluation with all warnings suppressed...\nüéØ ENHANCED ML MODEL EVALUATION\n==================================================\nüîá All BLAS warnings completely disabled\nüîç Validating data...\n‚úÖ Data validation passed\n   Training samples: 640\n   Test samples: 160\n   Features: 25088\n   Classes: 4\n\n==================== Logistic Regression ====================\nüîµ Training Logistic Regression...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Older code","metadata":{}}]}